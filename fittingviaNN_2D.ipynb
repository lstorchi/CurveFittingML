{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import commonmodules as cm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"N2H2_2D.xlsx\"\n",
    "excf = pd.ExcelFile(filename)\n",
    "debug = False\n",
    "print(excf.sheet_names)\n",
    "\n",
    "df1 = pd.read_excel(excf, \"dv=1\")\n",
    "df2 = pd.read_excel(excf, \"dv=2\")\n",
    "df3 = pd.read_excel(excf, \"dv=3\")\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cE = Collision Energy\n",
    "dE = Delta E \n",
    "cS = Cross Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x = {}\n",
    "x_s = {}\n",
    "y = {} \n",
    "y_s = {}\n",
    "scalerx = {}\n",
    "scalery = {}\n",
    "x1map_toreal = {}\n",
    "f1set = {}\n",
    "\n",
    "x[\"1_v_cE\"] = df1[['v', 'cE']].values\n",
    "x[\"1_dE_cE\"] = df1[['dE', 'cE']].values\n",
    "y[\"1\"] = np.log10(df1[[\"cS\"]].values)\n",
    "\n",
    "x[\"2_v_cE\"] = df2[['v', 'cE']].values\n",
    "x[\"2_dE_cE\"] = df2[['dE', 'cE']].values\n",
    "y[\"2\"] = np.log10(df2[[\"cS\"]].values)\n",
    "\n",
    "x[\"3_v_cE\"] = df3[['v', 'cE']].values\n",
    "x[\"3_dE_cE\"] = df3[['dE', 'cE']].values\n",
    "y[\"3\"] = np.log10(df3[[\"cS\"]].values)\n",
    "\n",
    "xkey = [\"1_v_cE\", \"1_dE_cE\", \\\n",
    "        \"2_v_cE\", \"2_dE_cE\", \\\n",
    "        \"3_v_cE\", \"3_dE_cE\"]\n",
    "\n",
    "ykey = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "for k in xkey:\n",
    "    scalerx[k] = MinMaxScaler()\n",
    "    scalerx[k].fit(x[k])\n",
    "    x_s[k] = scalerx[k].transform(x[k])\n",
    "\n",
    "    x1map = {}\n",
    "\n",
    "    for i, vn in enumerate(x_s[k][:,0]):\n",
    "        x1map[vn] = x[k][i,0]\n",
    "\n",
    "    x1map_toreal[k] = x1map\n",
    "    \n",
    "    f1set[k] = set(x_s[k][:,0])\n",
    "\n",
    "    if debug:\n",
    "        for i, xs in enumerate(x_s[k]):\n",
    "            print(xs, x[k][i])\n",
    "\n",
    "for k in ykey:\n",
    "    scalery[k] = MinMaxScaler()\n",
    "    scalery[k].fit(y[k])\n",
    "    y_s[k] = scalery[k].transform(y[k])\n",
    "\n",
    "    if debug:\n",
    "        for i, ys in enumerate(y_s[k]):\n",
    "            print(ys, y[k][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.optimizers as tko\n",
    "import tensorflow.keras.activations as tka\n",
    "import tensorflow.keras.losses as tkl\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelshapes = [[ 8,  8,  8,  8, 8],\n",
    "                [16, 16, 16, 16, 16],\n",
    "                [32, 32, 32, 32, 32],\n",
    "                [64, 64, 64, 64, 64],\n",
    "                [128, 128, 128, 128, 128],\n",
    "                [ 8,  8,  8,  8], \n",
    "                [16, 16, 16, 16],\n",
    "                [32, 32, 32, 32],\n",
    "                [64, 64, 64, 64],\n",
    "                [128, 128, 128, 128],\n",
    "                [ 8,  8,  8], \n",
    "                [16, 16, 16],\n",
    "                [32, 32, 32],\n",
    "                [64, 64, 64],\n",
    "                [128, 128, 128],\n",
    "                [ 8,  8], \n",
    "                [16, 16],\n",
    "                [32, 32],\n",
    "                [64, 64], \n",
    "                [128, 128]]\n",
    "epochs = 20\n",
    "batch_sizes = [10, 25, 50, 100]\n",
    "\n",
    "for xk in xkey:\n",
    "    yk = xk.split(\"_\")[0]\n",
    "    f1 = xk.split(\"_\")[1]\n",
    "    f2 = xk.split(\"_\")[2]\n",
    "\n",
    "    for modelshape in modelshapes:\n",
    "        for batch_size in batch_sizes:\n",
    "\n",
    "            thefirst = True\n",
    "\n",
    "            testmses  = []\n",
    "            testr2s   = []\n",
    "            trainmses = []\n",
    "            trainr2s  = []\n",
    "\n",
    "            print (\" xK , ModelShape , BatchSize , avg TrainMSE , avg TrainR2,  avg TestMSE ,avg TestR2 \")\n",
    "\n",
    "            for x1 in f1set[xk]:\n",
    "                train_x, test_x, train_y, test_y = cm.test_train_split (0, [x1], x_s[xk], y_s[yk])\n",
    "                \n",
    "                if thefirst:\n",
    "                    model = cm.buildmodel(modelshape, inputshape=2)\n",
    "                    #print(model.summary())\n",
    "                    history = model.fit(train_x, train_y, epochs=epochs,  batch_size=batch_size, \\\n",
    "                        verbose=0)\n",
    "                    thefirst = False\n",
    "  \n",
    "                model = cm.buildmodel(modelshape, inputshape=2)\n",
    "                history = model.fit(train_x, train_y, epochs=epochs,  batch_size=batch_size, \\\n",
    "                    verbose=0)\n",
    "            \n",
    "                test_x_sp = scalerx[xk].inverse_transform(test_x)\n",
    "                pred_y = model.predict(test_x, verbose=0)\n",
    "                pred_y_sb = scalery[yk].inverse_transform(pred_y)\n",
    "                test_y_sb = scalery[yk].inverse_transform(test_y)\n",
    "\n",
    "                testmse = metrics.mean_absolute_error(test_y_sb, pred_y_sb)\n",
    "                testr2 = metrics.r2_score(test_y_sb, pred_y_sb)\n",
    "                testmses.append(testmse)\n",
    "                testr2s.append(testr2)\n",
    "\n",
    "                pred_y = model.predict(train_x, verbose=0)\n",
    "                pred_y_sb = scalery[yk].inverse_transform(pred_y)\n",
    "                train_y_sb = scalery[yk].inverse_transform(train_y)\n",
    "                train_x_sp = scalerx[xk].inverse_transform(train_x)\n",
    "\n",
    "                trainmse = metrics.mean_absolute_error(train_y_sb, pred_y_sb)\n",
    "                trainr2 = metrics.r2_score(train_y_sb, pred_y_sb)\n",
    "                trainmses.append(trainmse)\n",
    "                trainr2s.append(trainr2)\n",
    "\n",
    "            print (xk, modelshape , batch_size , \\\n",
    "                   np.average(trainmses), \\\n",
    "                   np.average(trainr2s), \\\n",
    "                   np.average(testmses), \\\n",
    "                   np.average(testr2s))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelshape = [32, 32, 32, 32]\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "for xk in xkey:\n",
    "    yk = xk.split(\"_\")[0]\n",
    "    f1 = xk.split(\"_\")[1]\n",
    "    f2 = xk.split(\"_\")[2]\n",
    "    ofp = open(\"vremoved_NN_\"+xk+\".csv\", \"w\")\n",
    "\n",
    "    thefirst = True\n",
    "\n",
    "    print (f1+\" Removed , Test MSE , Test R2 , Train MSE , Train R2\")\n",
    "    print (f1+\" Removed , Test MSE , Test R2 , Train MSE , Train R2\", file=ofp)\n",
    "    for x1 in f1set[xk]:\n",
    "        train_x, test_x, train_y, test_y = cm.test_train_split (0, [x1], x_s[xk], y_s[yk])\n",
    "\n",
    "        if thefirst:\n",
    "            model = cm.buildmodel(modelshape, inputshape=2)\n",
    "            #print(model.summary())\n",
    "            history = model.fit(train_x, train_y, epochs=epochs,  batch_size=batch_size, \\\n",
    "                verbose=0)\n",
    "            thefirst = False\n",
    "\n",
    "        model = cm.buildmodel(modelshape, inputshape=2)\n",
    "        history = model.fit(train_x, train_y, epochs=epochs,  batch_size=batch_size, \\\n",
    "            verbose=0)\n",
    "        \n",
    "        ofptest = open(\"vremoved_NN_\"+str(x1map_toreal[xk][x1])+\"_\"+xk+\"_test.csv\", \"w\")\n",
    "        print (\" \"+f1+\" , \"+f2+\" , y , y_pred \", file=ofptest)\n",
    "        test_x_sp = scalerx[xk].inverse_transform(test_x)\n",
    "        pred_y = model.predict(test_x, verbose=0)\n",
    "        pred_y_sb = scalery[yk].inverse_transform(pred_y)\n",
    "        test_y_sb = scalery[yk].inverse_transform(test_y)\n",
    "        for i, yt in enumerate(test_y_sb):\n",
    "            print (\" %3d , %3d , %10.8e , %10.8e  \"%(test_x_sp[i,0], \n",
    "                                             test_x_sp[i,1],\n",
    "                                             yt,\n",
    "                                             pred_y_sb[i]), file=ofptest, flush=True)\n",
    "        #plt.scatter(test_y_sb, pred_y_sb)\n",
    "        #plt.show()\n",
    "        testmse = metrics.mean_absolute_error(test_y_sb, pred_y_sb)\n",
    "        testr2 = metrics.r2_score(test_y_sb, pred_y_sb)\n",
    "        ofptest.close()\n",
    "\n",
    "        ofptrain = open(\"vremoved_NN_\"+str(x1map_toreal[xk][v])+\"_\"+xk+\"_train.csv\", \"w\")\n",
    "        print (\" \"+f1+\" , \"+f2+\" , y , y_pred  \", file=ofptrain)\n",
    "        pred_y = model.predict(train_x, verbose=0)\n",
    "        pred_y_sb = scalery[yk].inverse_transform(pred_y)\n",
    "        train_y_sb = scalery[yk].inverse_transform(train_y)\n",
    "        train_x_sp = scalerx[xk].inverse_transform(train_x)\n",
    "        for i, yt in enumerate(train_y_sb):\n",
    "            print (\" %3d , %3d , %10.8e , %10.8e  \"%(train_x_sp[i,0], \n",
    "                                             train_x_sp[i,1],\n",
    "                                             yt,\n",
    "                                             pred_y_sb[i]), file=ofptrain, flush=True)\n",
    "        #plt.scatter(train_y_sb, pred_y_sb)\n",
    "        #plt.show()\n",
    "        trainmse = metrics.mean_absolute_error(train_y_sb, pred_y_sb)\n",
    "        trainr2 = metrics.r2_score(train_y_sb, pred_y_sb)\n",
    "        ofptrain.close()\n",
    "    \n",
    "        print(\"%3d , %10.6e , %10.6f , %10.6e , %10.6f\"%(x1map_toreal[xk][v], testmse, testr2, \\\n",
    "                                                       trainmse,  trainr2))\n",
    "    \n",
    "        print(\"%3d , %10.6e , %10.6f , %10.6e , %10.6f\"%(x1map_toreal[xk][v], testmse, testr2, \\\n",
    "                                                       trainmse,  trainr2), file=ofp)\n",
    "    \n",
    "ofp.close()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7411e63421f098f1bf3ed11ee17c34ffcc3ddc87944e7f4685f5b8c2980583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
